
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class QuestionGenerator:
    _instance = None  

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(QuestionGenerator, cls).__new__(cls)
            cls._instance._initialize_model()
        return cls._instance

    def _initialize_model(self):
        """ ðŸ“Œ **AI modelini yalnÄ±zca bir kez yÃ¼kler** """
        model_id = "google/gemma-2-2b-it"
        device = "mps" if torch.backends.mps.is_available() else "cpu" 
        
        print("ðŸš€ **AI modeli yÃ¼kleniyor, lÃ¼tfen bekleyin...**")

        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_id)
            self.tokenizer.pad_token = self.tokenizer.eos_token  

            self.model = AutoModelForCausalLM.from_pretrained(
                model_id, 
                torch_dtype=torch.float16 if device != "cpu" else torch.float32,  
                device_map=device
            )

            self.device = device
            print(f"âœ… **AI modeli baÅŸarÄ±yla yÃ¼klendi! (Device: {device})**")

        except Exception as e:
            print(f"ðŸš¨ Model yÃ¼klenirken hata oluÅŸtu: {e}")
            self.model = None

    def generate_questions(self, form_category, existing_questions, num_questions=5):
        """
        ðŸ“Œ **Formun kategorisine gÃ¶re AI destekli sorular Ã¼retir.**
        - Form baÅŸlÄ±ÄŸÄ±na uygun sorular Ã¼retir.
        - AI Ã¶nerdiÄŸi sorularÄ±n formdaki mevcut sorularla aynÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        """
        if self.model is None:
            print("ðŸš¨ **Model yÃ¼klenmedi! Soru Ã¼retilemiyor.**")
            return ["AI model is not loaded. Unable to generate questions."]

        prompt = f"""
        Generate {num_questions} unique, relevant questions specifically for a form titled "{form_category}".
        The questions should match the purpose of the form and provide useful input from the user.

        Example format:
        - If it's an "Appointment Request Form":
          1. What is the purpose of your appointment?
          2. What date and time are you available?
          3. Do you have any special requests for the appointment?
        
        - If it's a "Customer Feedback Form":
          1. How would you rate our service?
          2. What can we improve?
          3. Would you recommend us to others?

        Ensure that:
        - Each question is clearly written and numbered (1., 2., 3., etc.).
        - The response contains ONLY the questions (no additional text or explanations).
        """

        try:
            print(f"ðŸ“Œ **AI'ya GÃ¶nderilen Prompt:**\n{prompt}")  # ðŸ”¹ **GÃ¶nderilen prompt'u logla**

            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=250)  # ðŸ”¹ **YanÄ±tÄ± uzat**

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            print(f"ðŸ“Œ **Raw AI Output:**\n{generated_text}")  # ðŸ”¹ **YanÄ±tÄ± logla**

            # ðŸ“Œ **YanÄ±tÄ± temizleyelim**
            ai_generated_questions = []
            for line in generated_text.split("\n"):
                line = line.strip("-â€¢ ").strip()
                if line and line[0].isdigit():  
                    ai_generated_questions.append(line)

            if not ai_generated_questions:
                print("ðŸš¨ **AI'dan geÃ§erli soru alÄ±namadÄ±!**")
                return ["AI failed to generate proper questions. Please try again."]

            # ðŸ“Œ **Zaten formda var olan sorularÄ± kaldÄ±r**
            existing_questions_lower = {q.lower().strip() for q in existing_questions}
            filtered_questions = [q for q in ai_generated_questions if q.lower().strip() not in existing_questions_lower]

            # ðŸ“Œ **EÄŸer AI uygun soru Ã¼retmezse, mantÄ±klÄ± varsayÄ±lan sorular Ã¶ner**
            if not filtered_questions:
                print("ðŸš¨ **AI suggested questions overlap with existing ones. Using fallback questions.**")
                return self.get_fallback_questions(form_category)

            return filtered_questions[:num_questions]

        except Exception as e:
            print(f"ðŸš¨ AI Error Details: {str(e)}")
            return ["AI question generation failed due to an internal error."]

    def get_fallback_questions(self, form_category):
        """
        ðŸ“Œ **EÄŸer AI uygun soru Ã¼retemezse, form baÅŸlÄ±ÄŸÄ±na uygun varsayÄ±lan sorular dÃ¶ndÃ¼r.**
        """
        fallback_questions = {
            "Appointment Request Form": [
                "1. What is the purpose of your appointment?",
                "2. What date and time are you available?",
                "3. Do you have any special requests for the appointment?"
            ],
            "Customer Feedback Form": [
                "1. How would you rate our service?",
                "2. What can we improve?",
                "3. Would you recommend us to others?"
            ],
            "Job Application Form": [
                "1. What is your highest level of education?",
                "2. Do you have relevant work experience for this position?",
                "3. Why do you want to work for our company?"
            ],
            "Survey Form": [
                "1. How frequently do you use our service?",
                "2. What features do you find most valuable?",
                "3. What improvements would you suggest?"
            ]
        }

        return fallback_questions.get(form_category, ["1. What information would you like to provide?", "2. What is your main concern?", "3. How can we assist you?"])
    
    def generate_questions(self, form_category, existing_questions, num_questions=5):
        """
        ðŸ“Œ **Formun kategorisine gÃ¶re AI destekli sorular Ã¼retir.**
        - Form baÅŸlÄ±ÄŸÄ±na uygun sorular Ã¼retir.
        - AI Ã¶nerdiÄŸi sorularÄ±n formdaki mevcut sorularla aynÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder.
        """
        if self.model is None:
            print("ðŸš¨ **Model yÃ¼klenmedi! Soru Ã¼retilemiyor.**")
            return ["AI model is not loaded. Unable to generate questions."]

        prompt = f"""
        Generate {num_questions} unique, relevant questions specifically for a form titled "{form_category}".
        The questions should match the purpose of the form and provide useful input from the user.

        Ensure that:
        - Each question is clearly written and numbered (1., 2., 3., etc.).
        - The response contains ONLY the questions (no additional text or explanations).
        """

        try:
            print(f"ðŸ“Œ **AI'ya GÃ¶nderilen Prompt:**\n{prompt}")

            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=250)  

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            print(f"ðŸ“Œ **Raw AI Output:**\n{generated_text}")

            # ðŸ“Œ **YanÄ±tÄ± temizleyelim**
            ai_generated_questions = []
            for line in generated_text.split("\n"):
                line = line.strip("-â€¢ ").strip()
                if line and line[0].isdigit():  
                    ai_generated_questions.append(line)

            if not ai_generated_questions:
                print("ðŸš¨ **AI'dan geÃ§erli soru alÄ±namadÄ±!**")
                return ["AI failed to generate proper questions. Please try again."]

            # ðŸ“Œ **Zaten formda var olan sorularÄ± kaldÄ±r**
            existing_questions_lower = {q.lower().strip() for q in existing_questions}
            filtered_questions = [q for q in ai_generated_questions if q.lower().strip() not in existing_questions_lower]

            if not filtered_questions:
                print("ðŸš¨ **AI suggested questions overlap with existing ones. Using fallback questions.**")
                return self.get_fallback_questions(form_category)

            return filtered_questions[:num_questions]

        except Exception as e:
            print(f"ðŸš¨ AI Error Details: {str(e)}")
            return ["AI question generation failed due to an internal error."]

    def generate_question_from_user_input(self, user_question, form_purpose):
        """
        ðŸ“Œ KullanÄ±cÄ±nÄ±n girdisine gÃ¶re form baÅŸlÄ±ÄŸÄ±na uygun AI destekli yeni bir soru Ã¼retir.
        """
        if self.model is None:
            print("ðŸš¨ **Model yÃ¼klenmedi! Soru Ã¼retilemiyor.**")
            return ["AI model is not loaded. Unable to generate questions."]

        # ðŸ“Œ **TutarlÄ± Prompt FormatÄ±**
        prompt = f"""
        The user has asked: "{user_question}".
        Generate a single, unique question that can be added to a form titled "{form_purpose}".
        Your response should **ONLY** contain the question and **nothing else**.
        - Do **NOT** provide explanations.
        - Do **NOT** include answer choices.
        - Do **NOT** use labels like "Question:", "Options:", or "Explanation:".
        - The output should be a **single, well-formed question** ending with a question mark (?).
        """

        try:
            print(f"ðŸ“Œ **AI'ya GÃ¶nderilen Prompt:**\n{prompt}")

            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=100)

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            print(f"ðŸ“Œ **Raw AI Output:**\n{generated_text}")

            # ðŸ“Œ **YanÄ±ttan yalnÄ±zca ilk soru cÃ¼mlesini Ã§ek**
            lines = generated_text.split("\n")
            question = None

            for line in lines:
                line = line.strip("-â€¢ ").strip()
                if line.endswith("?"):  # Ä°lk geÃ§en soru cÃ¼mlesini al
                    question = line
                    break  # Ä°lk soruyu bulunca dur

            if not question:
                print("ðŸš¨ **AI uygun bir soru Ã¼retmedi. VarsayÄ±lan soru kullanÄ±lÄ±yor.**")
                return ["Could you please provide more details on this topic?"]

            return [question]  # AI'nÄ±n Ã¼rettiÄŸi ilk soruyu dÃ¶ndÃ¼r

        except Exception as e:
            print(f"ðŸš¨ AI Error Details: {str(e)}")
            return ["AI question generation failed due to an internal error."]


    def generate_question_from_prompt(self, prompt):
        """
        ðŸ“Œ KullanÄ±cÄ±nÄ±n girdisine gÃ¶re AI destekli yeni bir soru Ã¼retir.
        """
        if self.model is None:
            print("ðŸš¨ **Model yÃ¼klenmedi! Soru Ã¼retilemiyor.**")
            return ["AI model is not loaded. Unable to generate questions."]

        try:
            print(f"ðŸ“Œ **AI'ya GÃ¶nderilen Prompt:**\n{prompt}")

            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)

            with torch.no_grad():
                outputs = self.model.generate(**inputs, max_new_tokens=100)

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

            print(f"ðŸ“Œ **Raw AI Output:**\n{generated_text}")

            # ðŸ“Œ **YanÄ±ttan yalnÄ±zca ilk soru cÃ¼mlesini Ã§ek**
            lines = generated_text.split("\n")
            question = None

            for line in lines:
                line = line.strip("-â€¢ ").strip()
                if line.endswith("?"):  # Ä°lk geÃ§en soru cÃ¼mlesini al
                    question = line
                    break  # Ä°lk soruyu bulunca dur

            if not question:
                print("ðŸš¨ **AI uygun bir soru Ã¼retmedi. VarsayÄ±lan soru kullanÄ±lÄ±yor.**")
                return ["Could you please provide more details on this topic?"]

            return [question]  # AI'nÄ±n Ã¼rettiÄŸi ilk soruyu dÃ¶ndÃ¼r

        except Exception as e:
            print(f"ðŸš¨ AI Error Details: {str(e)}")
            return ["AI question generation failed due to an internal error."]


# âœ… **BaÄŸÄ±msÄ±z Test iÃ§in**
if __name__ == "__main__":
    question_gen = QuestionGenerator()

    print(f"ðŸ“Œ **Testing suggest_question_variants()**")
    user_question = "Where do you live?"
    improved_questions = question_gen.suggest_question_variants(user_question)

    print("ðŸ”¹ **AI Destekli Alternatif Sorular:**")
    for i, q in enumerate(improved_questions, 1):
        print(f"{i}. {q}")